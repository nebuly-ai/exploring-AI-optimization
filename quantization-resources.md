
<p align="center">
  <a href="https://discord.gg/RbeQMu886J">Join the community</a> •
  <a href="https://github.com/nebuly-ai/learning-AI-optimization#contribute">Contribute to the library</a>
</p>


<img height="25" width="100%" src="https://user-images.githubusercontent.com/83510798/171454644-d4b980bc-15ab-4a31-847c-75c36c5bd96b.png">


# Resources

Awesome resource collection on quantization techniques.

- <a href="#literature-reviews">Literature reviews</a> and <a href="#papers">papers</a>
- <a href="#courses-webinars-and-blogs">Courses, webinars and blogs</a>

If you still didn't, check [Overview page on quantization](https://github.com/nebuly-ai/learning-AI-optimization/blob/main/quantization-resources) or [Exploring AI optimization](https://github.com/nebuly-ai/exploring-AI-optimization) to start getting into it. 



## Literature reviews and papers
Legend: 
- ✏️  100-499 citations, ✏️✏️ $\geq$ 500 citations
- ⭐  100-249 stars, ⭐⭐ $\geq$ 250 stars

Sorting: chronological/alphabetic order
<br> 

### Literature reviews

- A Comprehensive Survey on Model Quantization for Deep Neural Networks [[paper](https://arxiv.org/abs/2205.07877)]
- A Mean Field Theory of Quantized Deep Networks: The Quantization-Depth Trade-Off [[NeurIPS](https://proceedings.neurips.cc/paper/2019/hash/38ef4b66cb25e92abe4d594acb841471-Abstract.html)]
- A Survey of Quantization Methods for Efficient Neural Network Inference [[✏️paper](https://arxiv.org/abs/2103.13630)]
- Analysis of Quantized Models [[ICLR](https://openreview.net/forum?id=ryM_IoAqYX)]
- Binary Neural Networks: A Survey [[✏️paper](https://www.sciencedirect.com/science/article/abs/pii/S0031320320300856)]
- Compression of Deep Learning Models for Text: A Survey [[✏️paper](https://arxiv.org/pdf/2008.05221.pdf)]


### Papers
2022
- 8-bit Optimizers via Block-wise Quantization [[ICLR](https://arxiv.org/abs/2110.02861)]
- F8Net: Fixed-Point 8-bit Only Multiplication for Network Quantization [[ICLR](https://arxiv.org/abs/2202.05239)]
- FQ-ViT: Post-Training Quantization for Fully Quantized Vision Transformer [[IJCAI](https://arxiv.org/abs/2111.13824)][[github](https://github.com/megvii-research/FQ-ViT)]
- Learnable Lookup Table for Neural Network Quantization [[CVPR](https://openaccess.thecvf.com/content/CVPR2022/html/Wang_Learnable_Lookup_Table_for_Neural_Network_Quantization_CVPR_2022_paper.html)]
- MultiQuant: Training Once for Multi-bit Quantization of Neural Networks [[paper](https://www.ijcai.org/proceedings/2022/0504.pdf)]
- Q-ViT: Fully Differentiable Quantization for Vision Transformer [[paper](https://arxiv.org/abs/2201.07703)]
- QDrop: Randomly Dropping Quantization for Extremely Low-bit Post-Training Quantization [[paper](https://arxiv.org/abs/2203.05740)]
- RAPQ: Rescuing Accuracy for Power-of-Two Low-bit Post-training Quantization [[IJCAI](https://arxiv.org/abs/2204.12322)]
- SQuant: On-the-Fly Data-Free Quantization via Diagonal Hessian Approximation [[ICLR](https://arxiv.org/abs/2202.07471)]
- Toward Efficient Low-Precision Training: Data Format Optimization and Hysteresis Quantization [[ICLR](https://openreview.net/forum?id=3HJOA-1hb0e)]


2021
- A Winning Hand: Compressing Deep Networks Can Improve Out-of-Distribution Robustness [[NeurIPS](https://proceedings.neurips.cc/paper/2021/hash/0607f4c705595b911a4f3e7a127b44e0-Abstract.html)]
- Any-Precision Deep Neural Networks [[paper](https://ojs.aaai.org/index.php/AAAI/article/view/17286)]
- Binary Graph Neural Networks [[CVPR](https://openaccess.thecvf.com/content/CVPR2021/html/Bahri_Binary_Graph_Neural_Networks_CVPR_2021_paper.html)]
- BiPointNet: Binary Neural Network for Point Clouds [[paper](https://arxiv.org/abs/2010.05501)]
- BRECQ: Pushing the Limit of Post-Training Quantization by Block Reconstruction [[✏️paper](https://arxiv.org/abs/2102.05426)]
- BSQ: Exploring Bit-Level Sparsity for Mixed-Precision Neural Network Quantization [[ICLR](https://arxiv.org/abs/2102.10462)]
- Degree-Quant: Quantization-Aware Training for Graph Neural Networks [[ICLR](https://arxiv.org/abs/2008.05000)]
- Diversifying Sample Generation for Accurate Data-Free Quantization [[CVPR](https://openaccess.thecvf.com/content/CVPR2021/html/Zhang_Diversifying_Sample_Generation_for_Accurate_Data-Free_Quantization_CVPR_2021_paper.html)]
- FracBits: Mixed Precision Quantization via Fractional Bit-Widths [[paper](https://ojs.aaai.org/index.php/AAAI/article/view/17269)]
- High-Capacity Expert Binary Networks [[ICLR](https://arxiv.org/abs/2010.03558)]
- Incremental few-shot learning via vector quantization in deep embedded space [[ICLR](https://openreview.net/forum?id=3SV-ZePhnZM)]
- Learnable Companding Quantization for Accurate Low-bit Neural Networks [[CVPR](https://openaccess.thecvf.com/content/CVPR2021/html/Yamamoto_Learnable_Companding_Quantization_for_Accurate_Low-Bit_Neural_Networks_CVPR_2021_paper.html)]
- MQBench: Towards Reproducible and Deployable Model Quantization Benchmark [[paper](https://arxiv.org/abs/2111.03759)][[github](http://mqbench.tech/)]
- Multi-Prize Lottery Ticket Hypothesis: Finding Accurate Binary Neural Networks by Pruning A Randomly Weighted Network [[paper](https://arxiv.org/abs/2103.09377)]
- Network Quantization with Element-wise Gradient Scaling [[CVPR](https://openaccess.thecvf.com/content/CVPR2021/html/Lee_Network_Quantization_With_Element-Wise_Gradient_Scaling_CVPR_2021_paper.html)]
- OPQ: Compressing Deep Neural Networks with One-shot Pruning-Quantization [[AAAI](https://ojs.aaai.org/index.php/AAAI/article/view/16950)]
- Post-Training Quantization for Vision Transformer [[NeurIPS](https://proceedings.neurips.cc/paper/2021/hash/ec8956637a99787bd197eacd77acce5e-Abstract.html)]
- Post-­training Quantization with Multiple Points: Mixed Precision without Mixed Precision [[paper](https://ojs.aaai.org/index.php/AAAI/article/view/17054)]
- Post-Training Sparsity-Aware Quantization [[NeurIPS](https://proceedings.neurips.cc/paper/2021/hash/9431c87f273e507e6040fcb07dcb4509-Abstract.html)]
- ReCU: Reviving the Dead Weights in Binary Neural Networks [[ICCV](https://openaccess.thecvf.com/content/ICCV2021/html/Xu_ReCU_Reviving_the_Dead_Weights_in_Binary_Neural_Networks_ICCV_2021_paper.html)]
- Stochastic Precision Ensemble: Self‐Knowledge Distillation for Quantized Deep Neural Networks [[paper](https://ojs.aaai.org/index.php/AAAI/article/view/16839)]
- Training with Quantization Noise for Extreme Model Compression [[✏️paper](https://arxiv.org/abs/2004.07320)][[github](https://github.com/facebookresearch/fairseq/tree/main/examples/quant_noise)]
- TRQ: Ternary Neural Networks with Residual Quantization [[paper](https://ojs.aaai.org/index.php/AAAI/article/view/17036)]
- Zero-shot Adversarial Quantization [[CVPR](https://openaccess.thecvf.com/content/CVPR2021/html/Liu_Zero-Shot_Adversarial_Quantization_CVPR_2021_paper.html)]


2020
- APQ: Joint Search for Network Architecture, Pruning and Quantization Policy [[✏️CVPR](https://openaccess.thecvf.com/content_CVPR_2020/html/Wang_APQ_Joint_Search_for_Network_Architecture_Pruning_and_Quantization_Policy_CVPR_2020_paper.html)][⭐[github](https://github.com/mit-han-lab/apq)]
- AutoQ: Automated Kernel-Wise Neural Network Quantization [[paper](https://arxiv.org/abs/1902.05690)]
- Balanced Binary Neural Networks with Gated Residual [[IEEE](https://ieeexplore.ieee.org/abstract/document/9054599)]
- BiDet: An Efficient Binarized Object Detector [[CVPR](https://openaccess.thecvf.com/content_CVPR_2020/html/Wang_BiDet_An_Efficient_Binarized_Object_Detector_CVPR_2020_paper.html)][⭐[github](https://github.com/ZiweiWangTHU/BiDet)]
- BinaryDuo: Reducing Gradient Mismatch in Binary Activation Network by Coupling Binary Activations [[paper](https://arxiv.org/abs/2002.06517)]
- Differentiable Joint Pruning and Quantization for Hardware Efficiency [[ECCV](https://link.springer.com/chapter/10.1007/978-3-030-58526-6_16)]
- Forward and Backward Information Retention for Accurate Binary Neural Networks [[✏️CVPR](https://openaccess.thecvf.com/content_CVPR_2020/html/Qin_Forward_and_Backward_Information_Retention_for_Accurate_Binary_Neural_Networks_CVPR_2020_paper.html)][⭐[github](https://github.com/htqin/IR-Net)]
- GhostNet: More Features from Cheap Operations [[✏️✏️CVPR](https://openaccess.thecvf.com/content_CVPR_2020/html/Han_GhostNet_More_Features_From_Cheap_Operations_CVPR_2020_paper.html)][⭐⭐[github](https://github.com/huawei-noah/Efficient-AI-Backbones)]
- Gradient ℓ1 Regularization for Quantization Robustness [[ICLR](https://arxiv.org/abs/2002.07520)]
- Learned Step Size Quantization [[✏️ICLR](https://arxiv.org/abs/1902.08153)]
- Linear Symmetric Quantization of Neural Networks for Low-precision Integer Hardware [[ICML](https://openreview.net/pdf?id=H1lBj2VFPS)]
- MeliusNet: Can Binary Neural Networks Achieve MobileNet-level Accuracy? [[✏️paper](https://arxiv.org/abs/2001.05936)][⭐[github](https://github.com/hpi-xnor/BMXNet-v2)]
- Mixed Precision DNNs: All you need is a good parametrization [[✏️ICLR](https://arxiv.org/abs/1905.11452)][⭐[github](https://github.com/sony/ai-research-code)]
- PROFIT: A Novel Training Method for sub-4-bit MobileNet Models [[ECCV](https://link.springer.com/chapter/10.1007/978-3-030-58539-6_26)]
- Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT [[✏️AAAI](https://ojs.aaai.org/index.php/AAAI/article/view/6409)]
- ReActNet: Towards Precise Binary Neural Network with Generalized Activation Functions [[✏️ECCV](https://link.springer.com/chapter/10.1007/978-3-030-58568-6_9)]
- Riptide: Fast End-to-End Binarized Neural Networks [[MLSys](https://proceedings.mlsys.org/paper/2020/hash/2a79ea27c279e471f4d180b08d62b00a-Abstract.html)][⭐[github](https://github.com/jwfromm/Riptide)]
- Rotation Consistent Margin Loss for Efficient Low-Bit Face Recognition [[CVPR](https://openaccess.thecvf.com/content_CVPR_2020/html/Wu_Rotation_Consistent_Margin_Loss_for_Efficient_Low-Bit_Face_Recognition_CVPR_2020_paper.html)]
- Soft Weight-Sharing for Neural Network Compression [[✏️ICLR](https://arxiv.org/abs/1702.04008)]

2019
- Additive Powers-of-Two Quantization: A Non-uniform Discretization for Neural Networks [[✏️paper](https://arxiv.org/abs/1909.13144)][⭐[github](https://github.com/yhhhli/APoT_Quantization)]
- An Empirical study of Binary Neural Networks' Optimisation [[✏️ICLR](https://openreview.net/forum?id=rJfUCoR5KX)][[github](https://github.com/mil-ad/studying-binary-neural-networks)]
- ARM: Augment-REINFORCE-Merge Gradient for Stochastic Binary Networks [[✏️ICLR](https://arxiv.org/abs/1807.11143)]
- Binary Ensemble Neural Network: More Bits per Network or More Networks per Bit? [[✏️CVPR](https://openaccess.thecvf.com/content_CVPR_2019/html/Zhu_Binary_Ensemble_Neural_Network_More_Bits_per_Network_or_More_CVPR_2019_paper.html)]
- Circulant Binary Convolutional Networks: Enhancing the Performance of 1-Bit DCNNs With Circulant Back Propagation [[✏️CVPR](https://openaccess.thecvf.com/content_CVPR_2019/html/Liu_Circulant_Binary_Convolutional_Networks_Enhancing_the_Performance_of_1-Bit_DCNNs_CVPR_2019_paper.html)]
- Data-Free Quantization through Weight Equalization and Bias Correction [[✏️ICCV](https://openaccess.thecvf.com/content_ICCV_2019/html/Nagel_Data-Free_Quantization_Through_Weight_Equalization_and_Bias_Correction_ICCV_2019_paper.html)]
- Defensive Quantization: When Efficiency Meets Robustness [[✏️paper](https://arxiv.org/abs/1904.08444)]
- Differentiable Soft Quantization: Bridging Full-Precision and Low-Bit Neural Networks [[✏️ICCV](https://openaccess.thecvf.com/content_ICCV_2019/html/Gong_Differentiable_Soft_Quantization_Bridging_Full-Precision_and_Low-Bit_Neural_Networks_ICCV_2019_paper.html)]
- ECC: Platform-Independent Energy-Constrained Deep Neural Network Compression via a Bilinear Regression Model [[CVPR](https://openaccess.thecvf.com/content_CVPR_2019/html/Yang_ECC_Platform-Independent_Energy-Constrained_Deep_Neural_Network_Compression_via_a_Bilinear_CVPR_2019_paper.html)]
- Eyeriss v2: A Flexible Accelerator for Emerging Deep Neural Networks on Mobile Devices [[✏️✏️IEEE](https://ieeexplore.ieee.org/abstract/document/8686088)]
- Fully Quantized Network for Object Detection [[✏️CVPR](https://openaccess.thecvf.com/content_CVPR_2019/html/Li_Fully_Quantized_Network_for_Object_Detection_CVPR_2019_paper.html)]
- Fully Quantized Transformer for Machine Translation [[paper](V)]
- HAQ: Hardware-Aware Automated Quantization With Mixed Precision [[✏️✏️CVPR](https://openaccess.thecvf.com/content_CVPR_2019/html/Wang_HAQ_Hardware-Aware_Automated_Quantization_With_Mixed_Precision_CVPR_2019_paper.html)]
- Improving Neural Network Quantization without Retraining using Outlier Channel Splitting [[✏️PMLR](http://proceedings.mlr.press/v97/zhao19c.html)]
- Latent Weights Do Not Exist: Rethinking Binarized Neural Network Optimization [[✏️NeurIPS](https://proceedings.neurips.cc/paper/2019/hash/9ca8c9b0996bbf05ae7753d34667a6fd-Abstract.html)]
- Learning Channel-Wise Interactions for Binary Convolutional Neural Networks [[✏️CVPR](https://openaccess.thecvf.com/content_CVPR_2019/html/Wang_Learning_Channel-Wise_Interactions_for_Binary_Convolutional_Neural_Networks_CVPR_2019_paper.html)]
- Learning to Quantize Deep Networks by Optimizing Quantization Intervals With Task Loss [[✏️CVPR](https://openaccess.thecvf.com/content_CVPR_2019/html/Jung_Learning_to_Quantize_Deep_Networks_by_Optimizing_Quantization_Intervals_With_CVPR_2019_paper.html)]
- Per-Tensor Fixed-Point Quantization of the Back-Propagation Algorithm [[ICLR](https://arxiv.org/abs/1812.11732)]
- Qsparse-local-SGD: Distributed SGD with Quantization, Sparsification, and Local Computations [[✏️NeurIPS](https://proceedings.neurips.cc/paper/2019/hash/d202ed5bcfa858c15a9f383c3e386ab2-Abstract.html)]
- Quantization Networks [[✏️CVPR](https://openaccess.thecvf.com/content_CVPR_2019/html/Yang_Quantization_Networks_CVPR_2019_paper.html)][⭐[github](https://github.com/aliyun/alibabacloud-quantization-networks)]
- Regularizing Activation Distribution for Training Binarized Deep Networks [[✏️CVPR](https://openaccess.thecvf.com/content_CVPR_2019/html/Ding_Regularizing_Activation_Distribution_for_Training_Binarized_Deep_Networks_CVPR_2019_paper.html)]
- Same, Same But Different: Recovering Neural Network Quantization Error Through Weight Factorization [[✏️PMLR](https://proceedings.mlr.press/v97/meller19a.html)]
- SeerNet: Predicting Convolutional Neural Network Feature-Map Sparsity Through Low-Bit Quantization [[CVPR](https://openaccess.thecvf.com/content_CVPR_2019/html/Cao_SeerNet_Predicting_Convolutional_Neural_Network_Feature-Map_Sparsity_Through_Low-Bit_Quantization_CVPR_2019_paper.html)]
- Simultaneously Optimizing Weight and Quantizer of Ternary Neural Network Using Truncated Gaussian Approximation [[✏️CVPR](https://openaccess.thecvf.com/content_CVPR_2019/html/He_Simultaneously_Optimizing_Weight_and_Quantizer_of_Ternary_Neural_Network_Using_CVPR_2019_paper.html)]

2018
- Adaptive Quantization of Neural Networks [[ICLR](https://openreview.net/forum?id=SyOK1Sg0W)]
- Bi-Real Net: Enhancing the Performance of 1-bit CNNs With Improved Representational Capability and Advanced Training Algorithm [[✏️ECCV](https://openaccess.thecvf.com/content_ECCV_2018/html/zechun_liu_Bi-Real_Net_Enhancing_ECCV_2018_paper.html)][⭐[github](https://github.com/liuzechun/Bi-Real-net)]
- BinaryRelax: A Relaxation Approach For Training Deep Neural Networks With Quantized Weights [[✏️SIAM](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C5&q=BinaryRelax%3A+A+Relaxation+Approach+For+Training+Deep+Neural+Networks+With+Quantized+Weights&btnG=)]
- CLIP-Q: Deep Network Compression Learning by In-Parallel Pruning-Quantization [[✏️CVPR](Q_Deep_Network_CVPR_2018_paper)]
- Combinatorial Attacks on Binarized Neural Networks [[paper](https://arxiv.org/abs/1810.03538)]
- DoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with Low Bitwidth Gradients [[✏️✏️paper](https://arxiv.org/abs/1606.06160)]
- Explicit Loss-Error-Aware Quantization for Low-Bit Deep Neural Networks [[✏️CVPR](https://openaccess.thecvf.com/content_cvpr_2018/html/Zhou_Explicit_Loss-Error-Aware_Quantization_CVPR_2018_paper.html)]
- FINN-R: An End-to-End Deep-Learning Framework for Fast Exploration of Quantized Neural Networks [[✏️paper](https://dl.acm.org/doi/abs/10.1145/3242897)]
- FP-BNN: Binarized neural network on FPGA [[✏️paper](https://www.sciencedirect.com/science/article/abs/pii/S0925231217315655)]
- Heterogeneous Bitwidth Binarization in Convolutional Neural Networks [[NeurIPS](https://proceedings.neurips.cc/paper/2018/hash/1b36ea1c9b7a1c3ad668b8bb5df7963f-Abstract.html)]
- HitNet: Hybrid Ternary Recurrent Neural Network [[NeurIPS](https://proceedings.neurips.cc/paper/2018/hash/82cec96096d4281b7c95cd7e74623496-Abstract.html)]
- Learning Discrete Weights Using the Local Reparameterization Trick [[✏️ICLR](https://arxiv.org/abs/1710.07739)]
- Loss-aware Weight Quantization of Deep Networks [[✏️paper](https://arxiv.org/abs/1802.08635)]
- LQ-Nets: Learned Quantization for Highly Accurate and Compact Deep Neural Networks [[✏️ECCV](https://openaccess.thecvf.com/content_ECCV_2018/html/Dongqing_Zhang_Optimized_Quantization_for_ECCV_2018_paper.html)]
- Model compression via distillation and quantization [[✏️ICLR](https://arxiv.org/abs/1802.05668)][⭐⭐[github](https://github.com/antspy/quantized_distillation)]
- PACT: Parameterized Clipping Activation for Quantized Neural Networks [[✏️✏️paper](https://arxiv.org/abs/1805.06085)]
- ProxQuant: Quantized Neural Networks via Proximal Operators [[✏️paper](https://arxiv.org/abs/1810.00861)]
- Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference [[✏️✏️CVPR](https://openaccess.thecvf.com/content_cvpr_2018/html/Jacob_Quantization_and_Training_CVPR_2018_paper.html)]
- Quantizing deep convolutional networks for efficient inference: A whitepaper [[✏️✏️paper](https://arxiv.org/abs/1806.08342)]
- Relaxed Quantization for Discretized Neural Networks [[✏️paper](https://arxiv.org/abs/1810.01875)]
- Scalable Methods for 8-bit Training of Neural Networks [[✏️NeurIPS](https://proceedings.neurips.cc/paper/2018/hash/e82c4b19b8151ddc25d4d93baf7b908f-Abstract.html)]
- SIGNSGD: compressed optimisation for non-convex problems [[✏️✏️PMLR](https://proceedings.mlr.press/v80/bernstein18a.html)]

2017
- Apprentice: Using Knowledge Distillation Techniques To Improve Low-Precision Network Accuracy [[✏️paper](https://arxiv.org/abs/1711.05852)]
- Binarized Convolutional Landmark Localizers for Human Pose Estimation and Face Alignment with Limited Resources [[✏️ICCV](https://openaccess.thecvf.com/content_iccv_2017/html/Bulat_Binarized_Convolutional_Landmark_ICCV_2017_paper.html)]
- Deep Learning with Low Precision by Half-wave Gaussian Quantization [[✏️CVPR](https://openaccess.thecvf.com/content_cvpr_2017/html/Cai_Deep_Learning_With_CVPR_2017_paper.html)]
- Extremely Low Bit Neural Network: Squeeze the Last Bit Out with ADMM [[✏️paper](https://arxiv.org/abs/1707.09870)]
- FINN: A Framework for Fast, Scalable Binarized Neural Network Inference [[✏️✏️paper](https://dl.acm.org/doi/abs/10.1145/3020078.3021744)]
- Incremental Network Quantization: Towards Lossless CNNs with Low-Precision Weights [[✏️✏️ICLR](https://arxiv.org/abs/1702.03044)][⭐[github](https://github.com/AojunZhou/Incremental-Network-Quantization)]
- Local Binary Convolutional Neural Networks [[✏️CVPR](https://openaccess.thecvf.com/content_cvpr_2017/html/Juefei-Xu_Local_Binary_Convolutional_CVPR_2017_paper.html)][[github](https://github.com/juefeix/lbcnn.torch)]
- Network Sketching: Exploiting Binary Structure in Deep CNNs [[✏️CVPR](https://openaccess.thecvf.com/content_cvpr_2017/html/Guo_Network_Sketching_Exploiting_CVPR_2017_paper.html)]
- QSGD: Communication-Efficient SGD via Gradient Quantization and Encoding [[✏️✏️NIPS](https://proceedings.neurips.cc/paper/2017/file/6c340f25839e6acdc73414517203f5f0-Paper.pdf)]
- Quantized Neural Networks Quantized Neural Networks: Training Neural Networks with Low Precision Weights and Activations [[✏️✏️paper](https://www.jmlr.org/papers/volume18/16-456/16-456.pdf)]

Before 2017
- Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1 [[✏️✏️paper](https://arxiv.org/abs/1602.02830)][⭐⭐[github](https://github.com/itayhubara/BinaryNet)]
- Bitwise Neural Networks [[✏️ICML](https://arxiv.org/abs/1601.06071)]
- Loss-aware Binarization of Deep Networks [[✏️paper](https://arxiv.org/abs/1611.01600)]
- Overcoming Challenges in Fixed Point Training of Deep Convolutional Networks [[ICML](https://arxiv.org/abs/1607.02241)]
- Quantized Convolutional Neural Networks for Mobile Devices [[✏️✏️CVPR](https://openaccess.thecvf.com/content_cvpr_2016/html/Wu_Quantized_Convolutional_Neural_CVPR_2016_paper.html)]
- BinaryConnect: Training Deep Neural Networks with binary weights during propagations [[✏️✏️NIPS](https://arxiv.org/abs/1511.00363)][⭐⭐[github](https://github.com/MatthieuCourbariaux/BinaryConnect)]
- Expectation Backpropagation: Parameter-Free Training of Multilayer Neural Networks with Continuous or Discrete Weights [[✏️NIPS](https://proceedings.neurips.cc/paper/2014/hash/076a0c97d09cf1a0ec3e19c7f2529f2b-Abstract.html)]
- Back to Simplicity: How to Train Accurate BNNs from Scratch? [[paper](https://arxiv.org/abs/1906.08637)][⭐[github](https://github.com/hpi-xnor/BMXNet-v2)]




## Courses, webinars and blogs

Webinars, video content
- [Codebasics, Quantization in deep learning (Tensorflow, Keras & Python)](https://www.youtube.com/watch?v=v1oHf1KV6kM)
- [Hailo, Quantization of Neural Networks – High Accuracy at Low Precision](https://www.youtube.com/watch?v=DhHyshhc1lY)
- [Sony, Downsizing Neural Networks by Quantization](https://www.youtube.com/watch?v=DDelqfkYCuo)
- [tinyML, A Practical Guide to Neural Network Quantization](https://www.youtube.com/watch?v=KASuxB3XoYQ)

Blogs, written content
- [How to accelerate and compress neural networks with quantization](https://towardsdatascience.com/how-to-accelerate-and-compress-neural-networks-with-quantization-edfbbabb6af7)
- [Matlab, Quantization of Deep Neural Networks](https://www.mathworks.com/help/deeplearning/ug/quantization-of-deep-neural-networks.html)
- [Quantization for Neural Networks](https://leimao.github.io/article/Neural-Networks-Quantization/)
- [TinyML, Neural Network Quantization](https://www.allaboutcircuits.com/technical-articles/neural-network-quantization-what-is-it-and-how-does-it-relate-to-tiny-machine-learning/#:~:text=What%20is%20Quantization%20for%20Neural,that%20they%20consume%20less%20memory.)




<img height="25" width="100%" src="https://user-images.githubusercontent.com/83510798/171454644-d4b980bc-15ab-4a31-847c-75c36c5bd96b.png">

<p align="center">
  <a href="https://discord.gg/RbeQMu886J">Join the community</a> •
  <a href="https://github.com/nebuly-ai/learning-AI-optimization#contribute">Contribute to the library</a>
</p>
