
<p align="center">
  <a href="https://discord.gg/RbeQMu886J">Join the community</a> â€¢
  <a href="#contribute">Contribute to the library</a>
</p>

<img height="25" width="100%" src="https://user-images.githubusercontent.com/83510798/171454644-d4b980bc-15ab-4a31-847c-75c36c5bd96b.png">


# Resources

Awesome resource collection on pruning techniques.

- Literature reviews and papers
- Courses, webinars, and blogs
- Open-source libraries

And check [overview page on quantization](https://github.com/emilecourthoud/learning-AI-optimization/blob/main/quantization-resources) f an overview of what quantization, as well as a mapping of quantization techniques, 
And check out [this page](https://github.com/emilecourthoud/learning-AI-optimization/blob/main/quantization-overview) for an overview of quantization process and a concept map of quantization techniques.


IMAGE

## Literature reviews and papers
Legenda:Â 
- âœï¸ Â More than 20 citations (20+, 50+, 100+)
- â­Â With GitHub code and more than 50 stars (100+, 300+, 1k+, 3k+)
Sorting: alphabetic order
<br>

### List of literature reviews

2017-2022
- ...
- ...

### List of papers
2022

2021
- Towards Accurate Post-training Network Quantization via Bit-Split and Stitching
- Differentiable Product Quantization for End-to-End Embedding Compression
- Multi-Precision Policy Enforced Training (MuPPET): A precision-switching strategy for quantised fixed-point training of CNNs
- Online Learned Continual Compression with Adaptive Quantization Modules
- Variational Bayesian Quantization
- Deep Molecular Programming: A Natural Implementation of Binary-Weight ReLU Neural Networks
- Training Binary Neural Networks through Learning with Noisy Supervision
- Training Binary Neural Networks using the Bayesian Learning Rule

2020
- Divide and Conquer: Leveraging Intermediate Feature Representations for Quantized Training of Neural Networks [[PMLR](https://arxiv.org/abs/1906.06033)]
- Up or Down? Adaptive Rounding for Post-Training Quantization [50+âœï¸ [PMLR](https://arxiv.org/abs/2004.10568)]

- 
2019


## Courses, webinars, blogs

Legenda: ğŸ¥‡ğŸ¥ˆğŸ¥‰ Combined project-quality score

2017-2022

- â€¦
- â€¦
- â€¦

Before 2017

- â€¦
- â€¦

### Libraries and resources

Legenda

- ğŸ£ New projectÂ *(less than 6 months old)*
- ğŸ’¤ Inactive projectÂ *(6 months no activity)*

Sorting: alphabetic order

List of libraries and resources
- ğŸ£Â Nebulgym G stars 151 - Accelerate AI training in a few lines of code without changing the training setup. Apache-2
- OpenVINOâ„¢ G stars 3.4k - Open-source toolkit for optimizing and deploying AI inference. Apache-2




<img height="25" width="100%" src="https://user-images.githubusercontent.com/83510798/171454644-d4b980bc-15ab-4a31-847c-75c36c5bd96b.png">

<p align="center">
  <a href="https://discord.gg/RbeQMu886J">Join the community</a> â€¢
  <a href="#contribute">Contribute to the library</a>
</p>
